{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (SA) with pretrained language model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load SA raw data using gluon.data and load pretrained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.gpu(2)\n",
    "\n",
    "with open('./wikitext2_vocab.json', 'r') as file:\n",
    "    wikitext2_vocab_json = file.read()\n",
    "vocab = gluon.text.vocab.Vocabulary.json_deserialize(wikitext2_vocab_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_model, vocab = gluon.model_zoo.text.standard_lstm_lm_650('wikitext-2', wikitext2_vocab, True, ctx = context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.model_zoo.text.lm import StandardRNN, AWDRNN\n",
    "lm_model = StandardRNN('lstm', len(vocab), 650, 650, 2, 0.5, True)\n",
    "lm_model.initialize(mx.init.Xavier(), ctx=context)\n",
    "lm_model.load_params('./standard_lstm_lm_650.params', ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dict = mx.nd.load('./model.params.29')\n",
    "# new_param_dict = {}\n",
    "# #standardrnn0_\n",
    "# for k, v in param_dict.items():\n",
    "#     nk = k.split('standardrnn0_')[1]\n",
    "#     new_param_dict[nk] = v\n",
    "# mx.nd.save('./standard_lstm_lm_650.params', new_param_dict)\n",
    "# print(new_param_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = gluon.data.text.IMDB(root='data/imdb', segment='train')\n",
    "test = gluon.data.text.IMDB(root='data/imdb', segment='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load user-defined tokenizer and Tokenize SA raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/msgpack_numpy.py:84: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  dtype=np.dtype(descr)).reshape(obj[b'shape'])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/msgpack_numpy.py:88: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  dtype=np.dtype(descr))[0]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = []\n",
    "train_labels = []\n",
    "for text, score in train:\n",
    "    train_tokenized.append(tokenizer(text))\n",
    "    train_labels.append(score)\n",
    "test_tokenized = []\n",
    "test_labels = []\n",
    "for text, score in test:\n",
    "    test_tokenized.append(tokenizer(text))\n",
    "    test_labels.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Map tokenized data into nd array based instances according to lm's training data vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples(x_raw_samples, vocab):\n",
    "    x_encoded_samples = []\n",
    "    for sample in x_raw_samples:\n",
    "        x_encoded_sample = []\n",
    "        for word in sample:\n",
    "            if word in vocab.token_to_idx:\n",
    "                x_encoded_sample.append(vocab.token_to_idx[word])\n",
    "            else:\n",
    "                x_encoded_sample.append(0)\n",
    "        x_encoded_samples.append(x_encoded_sample)            \n",
    "    return x_encoded_samples\n",
    "    \n",
    "def encode_labels(y_raw_samples):\n",
    "    y_encoded_samples = []\n",
    "    for score in y_raw_samples:\n",
    "        if score >= 7:\n",
    "            y_encoded_samples.append(1)\n",
    "        elif score <= 4:\n",
    "            y_encoded_samples.append(0)\n",
    "    return y_encoded_samples\n",
    "\n",
    "def pad_samples(x_encoded_samples, maxlen = 500, val = 0):\n",
    "    x_samples = []\n",
    "    for sample in x_encoded_samples:\n",
    "        if len(sample) > maxlen:\n",
    "            new_sample = sample[:maxlen]\n",
    "        else:\n",
    "            num_padding = maxlen - len(sample)\n",
    "            new_sample = sample\n",
    "            for i in range(num_padding):\n",
    "                new_sample.append(val)\n",
    "        x_samples.append(new_sample)\n",
    "    return x_samples\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded_train = encode_samples(train_tokenized, vocab)\n",
    "x_encoded_test = encode_samples(test_tokenized, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mx.nd.array(pad_samples(x_encoded_train, 500, 0), ctx = context)\n",
    "x_test = mx.nd.array(pad_samples(x_encoded_test, 500, 0), ctx = context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = mx.nd.array(encode_labels(train_labels), ctx = context)\n",
    "y_test = mx.nd.array(encode_labels(test_labels), ctx = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build SA classifier: pretrained lm encoder's hidden state as feature + binary dense layer as decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclass = 2\n",
    "# ##hyper parameters\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "model = gluon.nn.Sequential()\n",
    "with model.name_scope():\n",
    "    model.add(lm_model.embedding)\n",
    "    model.add(lm_model.encoder)\n",
    "    model.add(gluon.nn.HybridLambda('SequenceLast'))\n",
    "    model.add(gluon.nn.Dense(nclass, flatten=False))\n",
    "\n",
    "model[3].initialize(mx.init.Xavier(), ctx = context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                       {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Report evaluation results: train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(x_samples, y_samples):\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    for i, data in enumerate(x_samples):\n",
    "        data = mx.nd.reshape(data, (-2, batch_size)).as_in_context(context)\n",
    "        target = y_samples[i].as_in_context(context)\n",
    "        output = model(data)\n",
    "        predicts = mx.nd.argmax(output, axis=1)\n",
    "        accuracy.update(preds=predicts, labels=target)\n",
    "    return accuracy.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train SA model and evaluate on train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0. loss \n",
      "[0.06101925]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 1000. loss \n",
      "[0.00621813]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 2000. loss \n",
      "[0.00489169]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 3000. loss \n",
      "[0.00227122]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 4000. loss \n",
      "[0.0014558]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 5000. loss \n",
      "[0.00248873]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 6000. loss \n",
      "[0.00143651]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 7000. loss \n",
      "[0.0006521]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 8000. loss \n",
      "[0.00105492]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 9000. loss \n",
      "[0.1721312]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 10000. loss \n",
      "[0.0005992]\n",
      "<NDArray 1 @gpu(2)>\n",
      "Batch 11000. loss \n",
      "[0.00045945]\n",
      "<NDArray 1 @gpu(2)>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(x_train):\n",
    "        data = mx.nd.reshape(data, (-2, batch_size)).as_in_context(context)\n",
    "        target = y_train[i].as_in_context(context)\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            L = loss(output, target)\n",
    "        L.backward()\n",
    "        trainer.step(batch_size)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Batch %s. loss %s\"%(i, L))\n",
    "    train_accuracy = eval(x_train, y_train)\n",
    "    test_accuracy = eval(x_test, y_test)\n",
    "    print(\"Epoch %s. Train_acc %s, Test_acc %s\"%(epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
